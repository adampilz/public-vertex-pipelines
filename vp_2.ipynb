{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ed5c0-7044-4cc4-9187-cae20a13d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# setup\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de900673-1273-4747-afec-eba2baa6640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this notebook to run\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform as vertex\n",
    "from google_cloud_pipeline_components.experimental import vertex_notification_email as gcc_exp\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Metrics, Output, OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392a9ce3-aae2-45aa-ac64-38d8ab39448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}-vpipelines\"\n",
    "BUCKET_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_PATH}/pipeline_root\"\n",
    "PIPELINE_DATA = f\"{BUCKET_PATH}/data\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349b68d-e46c-4740-b63c-9f595bc5558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# BEGIN vertex pipelines\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0cb9b-0664-4778-81a3-d563b88f3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# create individual pipeline components, then specify the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0132181a-ac1f-440c-a8c8-d26d3a88e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BigQuery data and convert to CSV\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"component_create_dataset.yaml\"\n",
    ")\n",
    "def get_dataframe(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    bqclient = bigquery.Client(project=project_number)\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_table\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    dataframe = rows.to_dataframe(\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    "    dataframe = dataframe.sample(frac=1, random_state=2)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "483c7296-70ab-4782-92f7-45209071cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component to train a Scikit-learn model\n",
    "@component(\n",
    "    packages_to_install=[\"sklearn\", \"pandas\", \"numpy\", \"joblib\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"component_train_model.yaml\",\n",
    ")\n",
    "def sklearn_train(\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from numpy import mean, std\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold    \n",
    "    \n",
    "    from joblib import dump\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(dataset.path)\n",
    "    labels = df.pop(\"label\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    \n",
    "    # cross validation\n",
    "    skmodel = GradientBoostingClassifier()\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(skmodel, data, labels, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    # log cv metrics\n",
    "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "    metrics.log_metric(\"CV_accuracy_mean\", mean(n_scores))\n",
    "    metrics.log_metric(\"CV_accuracy_stdev\", std(n_scores))\n",
    "    \n",
    "    # fit the model on the whole dataset\n",
    "    skmodel = GradientBoostingClassifier()\n",
    "    skmodel.fit(data, labels)\n",
    "    \n",
    "    dump(skmodel, model.path + \".joblib\")\n",
    "    model.uri = model.uri  + \".joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573877c-2319-43bd-abea-69984228529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# define the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e3526f-6add-4485-ab54-9d98f43b52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pipeline\n",
    "@dsl.pipeline(name=\"my-pipeline-name\", description=\"my pipeline description\")\n",
    "\n",
    "# specify all the inputs the pipeline needs to run\n",
    "def my_pipeline(\n",
    "    bq_table: str = \"\",\n",
    "    output_data_path: str = \"data.csv\",\n",
    "    project_id: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    \n",
    "    # notification recipients\n",
    "    RECIPIENTS_LIST = [\"adampilz@google.com\"]\n",
    "    notify_email_task = gcc_exp.VertexNotificationEmailOp(recipients=RECIPIENTS_LIST)\n",
    "    \n",
    "    # when pipeline exits, send status notification\n",
    "    with dsl.ExitHandler(notify_email_task):\n",
    "        \n",
    "        # specify the nodes in the pipeline\n",
    "        dataset_task = get_dataframe(bq_table)\n",
    "        \n",
    "        #------comment out one of the following        \n",
    "        # default machine type\n",
    "        #model_task = sklearn_train(dataset_task.output)\n",
    "        # custom machine type\n",
    "        model_task = (sklearn_train(dataset_task.output)\n",
    "                      .set_cpu_limit('96')\n",
    "                      .set_memory_limit('624G')\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8a46c-2fbf-495e-b25b-722586d5dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# compile and run the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41dc81cd-bb4a-48d4-a63b-1a7427bcbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "my_package_path = 'my_vertex_pipeline_specification_file.json'\n",
    "compiler.Compiler().compile(pipeline_func=my_pipeline, package_path=my_package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cee43-c44e-4254-9316-bccb245b7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime parameters to pass to pipeline\n",
    "pipeline_params = {\"bq_table\": \"ap-alto-ml-1000.my_ds.tabular_binary_class_even_split_slim\"}\n",
    "\n",
    "# run the pipeline\n",
    "vertex.init(project=PROJECT_ID)\n",
    "\n",
    "job = vertex.PipelineJob(\n",
    "    display_name = \"my-pipeline-job-name\",\n",
    "    template_path = my_package_path,\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    parameter_values = pipeline_params,\n",
    "    enable_caching = False\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
