{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e34784-c18d-49ea-ac30-1fd4d20ba77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544fa65-e181-480a-a719-a9bd34ccbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# extract table from from BQ to GCS CSVs\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03118164-de22-4b11-896d-d524c8dd699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63158182-b56e-4228-8465-8ce35d4dfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET = \"ds_central1\"\n",
    "BQ_TABLE = \"tab_class_732inps_1600krows_tra\"\n",
    "TRAINING_DATA_BUCKET = \"ap-alto-ml-1000-bucket-us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dbf2784-339a-43c6-b141-e6d80e8b2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_rbddeba452ee2275_00000185a8933615_1 ... (108s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "! bq extract \\\n",
    "  --destination_format=CSV \\\n",
    "  --field_delimiter=',' \\\n",
    "  --print_header=false \\\n",
    "  {BQ_DATASET}.{BQ_TABLE} \\\n",
    "  gs://{TRAINING_DATA_BUCKET}/{BQ_TABLE}_*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a8bd8-a833-4aa2-92b7-08f1a82ffe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# combine CSVs into a single file\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82b0019-68c1-465c-98a9-707be3df179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380641d-0bdf-4e8f-aaee-4c7de30681ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file containing column headers\n",
    "l = list()\n",
    "l.append(\"label\")\n",
    "for i in range(732):\n",
    "    l.append(f\"double_field_{i}\")\n",
    "\n",
    "\n",
    "file_with_headers = f\"{BQ_TABLE}_headers.csv\"\n",
    "with open(file_with_headers, 'w', newline='') as csvfile:\n",
    "    w = csv.writer(csvfile, delimiter=',')\n",
    "    w.writerow(l)\n",
    "    \n",
    "# copy file to GCS dir with data\n",
    "! gsutil cp {file_with_headers} gs://{TRAINING_DATA_BUCKET}/{file_with_headers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "443586ef-d493-4119-8e49-02ee7eb6cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source files for GCS compose\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(TRAINING_DATA_BUCKET)\n",
    "\n",
    "sources = list()\n",
    "sources.append(bucket.get_blob(file_with_headers))\n",
    "\n",
    "# get blob names from GCS\n",
    "blobs = bucket.list_blobs()\n",
    "for b in bucket.list_blobs():\n",
    "    if b.name != file_with_headers:\n",
    "        sources.append(bucket.get_blob(b.name))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# chunk for GCS compose\n",
    "n = 30\n",
    "source_lists = [sources[i:i + n] for i in range(0, len(sources), n)]\n",
    "\n",
    "if len(source_lists) == 1:\n",
    "    destination_blob_name = f\"{BQ_TABLE}.csv\"\n",
    "    destination = bucket.blob(destination_blob_name)\n",
    "    destination.content_type = \"text/plain\"\n",
    "    destination.compose(source_lists[0])\n",
    "else:\n",
    "    for idx, source_list in enumerate(source_lists):\n",
    "        if idx==0 and idx < len(source_lists)-1:\n",
    "            destination_blob_name = f\"{BQ_TABLE}_temp_{idx}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose(source_list)\n",
    "        if idx > 0 and idx < len(source_lists)-1:\n",
    "            last_temp = [bucket.get_blob(destination.name)]\n",
    "            destination_blob_name = f\"{BQ_TABLE}_temp_{idx}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose( last_temp + source_list)\n",
    "        elif idx == len(source_lists)-1:\n",
    "            last_temp = [bucket.get_blob(destination.name)]\n",
    "            destination_blob_name = f\"{BQ_TABLE}.csv\"\n",
    "            destination = bucket.blob(destination_blob_name)\n",
    "            destination.content_type = \"text/plain\"\n",
    "            destination.compose( last_temp + source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c112057-a0fd-4488-a901-4ce2e0b959b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_TRAIN_URI = f\"gs://{TRAINING_DATA_BUCKET}/{BQ_TABLE}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8049a95-b69b-4765-8615-26b202c857f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# training pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efd9ca8-7825-4970-a683-51867af80be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this notebook to run\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform as vertex\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl, compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a579aaa-99d4-482e-9179-3a8ab6e610ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "P = ! gcloud config list --format 'value(core.project)'\n",
    "PROJECT_ID = P[0]\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}-vpipelines\"\n",
    "BUCKET_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_PATH}/pipeline_root\"\n",
    "PIPELINE_DATA = f\"{BUCKET_PATH}/data\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510583de-4c41-46ea-a627-8051d37c09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "@dsl.component(\n",
    "    base_image=\"us-central1-docker.pkg.dev/ap-alto-ml-1000/ap-docker-repo/autogluon_0.5.2-cpu-framework-ubuntu20.04-py3.8:latest\"\n",
    "    , packages_to_install=[\"protobuf<=3.18.1\"]\n",
    "    , output_component_file=\"component_autogluon_train.yaml\"\n",
    ")\n",
    "def autogluon_train(\n",
    "    dataset: dsl.Input[dsl.Dataset]\n",
    "    , model: dsl.Output[dsl.Model]\n",
    "):\n",
    "    \n",
    "    # build the default autogluon model\n",
    "    from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "    \n",
    "    label = 'label'\n",
    "    train_data = TabularDataset(dataset.path)\n",
    "    predictor = TabularPredictor(label=label, path=model.path).fit(train_data)\n",
    "    model.uri = model.uri  + \"/predictor.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a0143a-b5ac-48d4-872e-41a08c3e5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# define the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0fa215d-b36b-4053-9d44-dc73f2697b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pipeline\n",
    "@dsl.pipeline(name=\"autogluon-testing\", description=\"my pipeline description\")\n",
    "\n",
    "# specify all the inputs the pipeline needs to run\n",
    "def my_pipeline(\n",
    "    project_id: str\n",
    "    , region: str\n",
    "    , gcs_train_uri: str\n",
    "):\n",
    "\n",
    "    # specify the nodes in the pipeline\n",
    "    dataset_task = dsl.importer(\n",
    "        artifact_uri = gcs_train_uri,\n",
    "        artifact_class=dsl.Dataset,\n",
    "        reimport=True\n",
    "    )\n",
    "    \n",
    "    model_task = (autogluon_train(dataset_task.output)\n",
    "                 ).set_memory_limit('384G').set_cpu_limit('96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebd2c6b-62df-4772-aebc-3704c3a4c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# compile and run the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e635bec-dd61-4f87-b3d2-e70bccd62a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "my_package_path = 'my_vertex_pipeline_specification_file.json'\n",
    "compiler.Compiler().compile(pipeline_func=my_pipeline, package_path=my_package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6884cf1b-afcb-46af-9f1a-7f21b47c54a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/720376660491/locations/us-central1/pipelineJobs/autogluon-testing-20230112225517\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/720376660491/locations/us-central1/pipelineJobs/autogluon-testing-20230112225517')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/autogluon-testing-20230112225517?project=720376660491\n"
     ]
    }
   ],
   "source": [
    "# runtime parameters to pass to pipeline\n",
    "pipeline_params = { \"project_id\": PROJECT_ID\n",
    "                   , \"region\": REGION\n",
    "                   , \"gcs_train_uri\" : GCS_TRAIN_URI\n",
    "                  }\n",
    "\n",
    "# run the pipeline\n",
    "vertex.init(project=PROJECT_ID)\n",
    "\n",
    "job = vertex.PipelineJob(\n",
    "    display_name = \"my-pipeline-job-name\",\n",
    "    template_path = my_package_path,\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    parameter_values = pipeline_params,\n",
    "    enable_caching = False\n",
    ")\n",
    "\n",
    "job.submit(service_account = f\"sa-vertex-pipelines@{PROJECT_ID}.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
