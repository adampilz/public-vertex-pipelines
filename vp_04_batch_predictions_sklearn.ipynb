{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ed5c0-7044-4cc4-9187-cae20a13d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# setup\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de900673-1273-4747-afec-eba2baa6640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this notebook to run\n",
    "import sys, json\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform as vertex\n",
    "from google_cloud_pipeline_components.experimental import vertex_notification_email as gcc_exp\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Metrics, Output, OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "392a9ce3-aae2-45aa-ac64-38d8ab39448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "PROJECT_ID = \"your-project\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"bkt-{PROJECT_ID}-vpipelines\"\n",
    "BUCKET_PATH = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_PATH}/pipeline_root\"\n",
    "PIPELINE_DATA = f\"{BUCKET_PATH}/data\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349b68d-e46c-4740-b63c-9f595bc5558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# BEGIN vertex pipelines\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0cb9b-0664-4778-81a3-d563b88f3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# create individual pipeline components, then specify the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0132181a-ac1f-440c-a8c8-d26d3a88e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BigQuery data and convert to CSV\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"component_create_dataset.yaml\"\n",
    ")\n",
    "def get_dataframe(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    bqclient = bigquery.Client(project=project_number)\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_table\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    dataframe = rows.to_dataframe(\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    "    dataframe = dataframe.sample(frac=1, random_state=2)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f8d6198f-5ab1-4170-9d1b-e558f7d0969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score data using a custom sklearn model\n",
    "@component(\n",
    "    packages_to_install=[\"sklearn\", \"pandas\", \"numpy\"]\n",
    "    , base_image=\"python:3.9\"\n",
    "    , output_component_file=\"component_score_data.yaml\"\n",
    ")\n",
    "def score_data(\n",
    "    # inputs\n",
    "    model: Input[Model]\n",
    "    , to_score_data: Input[Dataset]\n",
    "    # outputs\n",
    "    , scored_data: Output[Dataset]\n",
    "):\n",
    "\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # load the model\n",
    "    skmodel = pickle.load(open(model.path , 'rb'))\n",
    "    \n",
    "    # load the data\n",
    "    df = pd.read_csv(to_score_data.path)\n",
    "    labels = df.pop(\"label\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    \n",
    "    # score data\n",
    "    predictions = skmodel.predict_proba(data)\n",
    "    \n",
    "    # write predictions    \n",
    "    predictions_file = 'predictions.csv'\n",
    "    num_cols = predictions.shape[1]\n",
    "    col_names = ','.join([f\"prob_{i}\" for i in range(num_cols)])\n",
    "    np.savetxt(  predictions_file\n",
    "               , predictions\n",
    "               , delimiter=','\n",
    "               , fmt='%f'\n",
    "               , header=col_names\n",
    "               , comments=\"\")\n",
    "    # metadata\n",
    "    scored_data.uri = scored_data.uri  + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c573877c-2319-43bd-abea-69984228529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "# define the pipeline\n",
    "#\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c3e3526f-6add-4485-ab54-9d98f43b52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"python-batch-predict-sklearn\"\n",
    "pipeline_description = \"my pipeline description\"\n",
    "\n",
    "# define a pipeline\n",
    "@dsl.pipeline(name=pipeline_name, description=pipeline_description)\n",
    "\n",
    "# specify all the inputs the pipeline needs to run\n",
    "def my_pipeline(\n",
    "    bq_table: str = \"\",\n",
    "    output_data_path: str = \"data.csv\",\n",
    "    project_id: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    # pipeline graph\n",
    "    \n",
    "    # import model from GCS location\n",
    "    importer_task = dsl.importer(\n",
    "        artifact_uri = 'gs://path-to-champion-model/model.pkl',\n",
    "        artifact_class = dsl.Model,\n",
    "        reimport = True,\n",
    "        metadata = {'model_name': \"sklearn GradientBoostingClassifier\"}\n",
    "    )\n",
    "    \n",
    "    # import data to score\n",
    "    dataset_task = get_dataframe(bq_table)\n",
    "    \n",
    "    # apply model to data\n",
    "    score_data_task = score_data(importer_task.output, dataset_task.output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "41dc81cd-bb4a-48d4-a63b-1a7427bcbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "my_package_path = 'my_vertex_pipeline_specification_file.json'\n",
    "compiler.Compiler().compile(pipeline_func=my_pipeline, package_path=my_package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cee43-c44e-4254-9316-bccb245b7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime parameters to pass to pipeline\n",
    "pipeline_params = {\"bq_table\": \"your-project.your-ds.your-table\"}\n",
    "\n",
    "# run the pipeline\n",
    "vertex.init(project=PROJECT_ID)\n",
    "\n",
    "job = vertex.PipelineJob(\n",
    "    display_name = pipeline_name\n",
    "    , template_path = my_package_path\n",
    "    , pipeline_root = PIPELINE_ROOT\n",
    "    , parameter_values = pipeline_params\n",
    "    , enable_caching = False\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m97"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
